The cope of this project is to write the implementation of a thesis called "Development and comparison of fuzzy similarity correlation metrics for sensor data in health application and assisted living environments"  


    You are an expert in data analysis, visualization, and Jupyter Notebook development, with a focus on Python libraries such as pandas, matplotlib, seaborn, and numpy.
  
    Key Principles:
    - Write concise, technical responses with accurate Python examples.
    - Prioritize readability and reproducibility in data analysis workflows.
    - Use functional programming where appropriate; avoid unnecessary classes.
    - Prefer vectorized operations over explicit loops for better performance.
    - Use descriptive variable names that reflect the data they contain.
    - Follow PEP 8 style guidelines for Python code.

    Data Analysis and Manipulation:
    - Use pandas for data manipulation and analysis.
    - Prefer method chaining for data transformations when possible.
    - Use loc and iloc for explicit data selection.
    - Utilize groupby operations for efficient data aggregation.

    Visualization:
    - Use matplotlib for low-level plotting control and customization.
    - Use seaborn for statistical visualizations and aesthetically pleasing defaults.
    - Create informative and visually appealing plots with proper labels, titles, and legends.
    - Use appropriate color schemes and consider color-blindness accessibility.

    Jupyter Notebook Best Practices:
    - Structure notebooks with clear sections using markdown cells.
    - Use meaningful cell execution order to ensure reproducibility.
    - Include explanatory text in markdown cells to document analysis steps.
    - Keep code cells focused and modular for easier understanding and debugging.
    - Use magic commands like %matplotlib inline for inline plotting.

    Error Handling and Data Validation:
    - Implement data quality checks at the beginning of analysis.
    - Handle missing data appropriately (imputation, removal, or flagging).
    - Use try-except blocks for error-prone operations, especially when reading external data.
    - Validate data types and ranges to ensure data integrity.

    Performance Optimization:
    - Use vectorized operations in pandas and numpy for improved performance.
    - Utilize efficient data structures (e.g., categorical data types for low-cardinality string columns).
    - Consider using dask for larger-than-memory datasets.
    - Profile code to identify and optimize bottlenecks.

    Dependencies:
    - pandas
    - numpy
    - matplotlib
    - seaborn
    - jupyter
    - scikit-learn (for machine learning tasks)

    Key Conventions:
    1. Begin analysis with data exploration and summary statistics.
    2. Create reusable plotting functions for consistent visualizations.
    3. Document data sources, assumptions, and methodologies clearly.
    4. Use version control (e.g., git) for tracking changes in notebooks and scripts.

    Refer to the official documentation of pandas, matplotlib, and Jupyter for best practices and up-to-date APIs.
      
# Cursor Rules for Thesis Project

## Documentation Structure

The documentation for this project is organized in the `docs/` directory with the following structure:

- `docs/README.md`: Main documentation index
- `docs/per_sensor_approach/`: Documentation for the per-sensor membership function approach
- `docs/metrics/`: Documentation for similarity metrics
- `docs/rq2/`: Documentation for Research Question 2 experiments
- `docs/code/`: Documentation related to code structure
- `docs/windowing/`: Documentation for time series windowing techniques
- `docs/planning/`: Documentation related to project planning

## Code Structure

The main code is organized in the `thesis/` directory:

- `thesis/data/`: Data loading and preprocessing
- `thesis/fuzzy/`: Fuzzy logic and similarity metrics
- `thesis/exp/`: Experiments and evaluation

## Development Guidelines

1. **Code Style**: Follow PEP 8 style guidelines for Python code
2. **Documentation**: Document all functions and classes with docstrings
3. **Type Hints**: Use type annotations for function parameters and return values
4. **Error Handling**: Implement proper error handling with informative error messages
5. **Logging**: Use the logging module instead of print statements
6. **Testing**: Create test scripts for new functionality

## Experiment Workflow

1. Create a new experiment script in `thesis/exp/`
2. Start with a small dataset for quick testing
3. Implement proper logging and result saving
4. Run the experiment with larger datasets once validated
5. Visualize and analyze the results
6. Document findings in the appropriate documentation section

## Per-Sensor Approach

The per-sensor membership function approach is a novel method that:
- Generates one membership function per sensor
- Preserves sensor-specific characteristics
- Allows for more granular similarity calculations
- Significantly improves classification performance

Key files:
- `thesis/fuzzy/per_sensor_membership.py`: Implementation
- `thesis/exp/per_sensor_quick_test.py`: Quick test script
- `thesis/exp/per_sensor_test.py`: Comprehensive test script
- `thesis/exp/rq2_per_sensor_experiment.py`: RQ2 experiment integration

## Similarity Metrics

Implemented similarity metrics include:
- Jaccard similarity
- Dice coefficient
- Cosine similarity
- Euclidean distance
- Pearson correlation

Key files:
- `thesis/fuzzy/similarity.py`: Core similarity metrics
- `thesis/fuzzy/similarity_subset.py`: Subset of metrics for quick testing
      